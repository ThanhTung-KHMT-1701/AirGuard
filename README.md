# AirGuard: Beijing Air Quality Monitoring & Prediction System

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE.txt)
[![Status](https://img.shields.io/badge/Status-Active-success.svg)]()

**AirGuard** lÃ  má»™t há»‡ thá»‘ng end-to-end pipeline phÃ¢n tÃ­ch vÃ  dá»± bÃ¡o cháº¥t lÆ°á»£ng khÃ´ng khÃ­ táº¡i Báº¯c Kinh (Beijing), sá»­ dá»¥ng dá»¯ liá»‡u tá»« 12 tráº¡m quan tráº¯c. Dá»± Ã¡n táº­p trung vÃ o ba má»¥c tiÃªu chÃ­nh:

1. ğŸ¯ **Dá»± bÃ¡o PM2.5** - Regression & ARIMA time series forecasting
2. ğŸš¨ **PhÃ¢n loáº¡i AQI** - Multi-class classification cho 6 levels (Good â†’ Hazardous)
3. ğŸ¤– **Semi-supervised Learning** - Cáº£i thiá»‡n model khi thiáº¿u labeled data

---

## ğŸ“‹ Má»¥c lá»¥c

- [Tá»•ng quan dá»± Ã¡n](#-tá»•ng-quan-dá»±-Ã¡n)
  - [Key insights](#-key-insights)
  - [Káº¿t quáº£ chÃ­nh](#-káº¿t-quáº£-chÃ­nh)
- [Cáº¥u trÃºc dá»± Ã¡n](#-cáº¥u-trÃºc-dá»±-Ã¡n)
- [Dataset](#-dataset)
- [CÃ i Ä‘áº·t mÃ´i trÆ°á»ng](#-cÃ i-Ä‘áº·t-mÃ´i-trÆ°á»ng)
- [Pipeline notebooks](#-pipeline-notebooks)
- [Káº¿t quáº£ chi tiáº¿t](#-káº¿t-quáº£-chi-tiáº¿t)
  - [1. Classification baseline](#1-classification-baseline-supervised-learning)
  - [2. Regression PM2.5](#2-regression-pm25-prediction)
  - [3. ARIMA forecasting](#3-arima-time-series-forecasting)
  - [4. Semi-supervised methods](#4-semi-supervised-learning-comparison)
- [Documentation](#-documentation)
- [Cháº¡y pipeline](#-cháº¡y-pipeline)
- [BÃ i há»c vÃ  insights](#-bÃ i-há»c-vÃ -insights)
- [TÃ¡c giáº£](#-tÃ¡c-giáº£)
- [License](#-license)

---

## ğŸŒŸ Tá»•ng quan dá»± Ã¡n

### ğŸ”‘ Key Insights

Sau khi thá»­ nghiá»‡m toÃ n diá»‡n vá»›i **6 phÆ°Æ¡ng phÃ¡p machine learning** (1 supervised baseline + 5 semi-supervised), chÃºng tÃ´i rÃºt ra nhá»¯ng insights quan trá»ng sau:

#### 1. Semi-supervised Learning hiá»‡u quáº£ vá»›i labeled data Ã­t

| Method | F1-Macro | Improvement vs Baseline | Use Case |
|--------|----------|------------------------|----------|
| **Supervised Baseline** | 0.472 | - | Baseline reference |
| **Self-Training** | 0.680 | **+44.1%** | âœ… General purpose, scalable |
| **Co-Training** | 0.710 | **+50.4%** | âœ… Best vá»›i 2 independent views |
| **Label Propagation** | 0.860* | **+82.2%** | âœ… Small data, binary only |
| **Label Spreading** | 0.870* | **+84.3%** | âœ… Best accuracy, binary only |
| **Dynamic Threshold** | 0.685 | **+45.1%** | âœ… Best cho imbalanced data |

\*Graph-based methods sá»­ dá»¥ng binary classification (Healthy vs Unhealthy)

#### 2. Model confidence áº£nh hÆ°á»Ÿng lá»›n Ä‘áº¿n SSL performance

**PhÃ¡t hiá»‡n quan trá»ng**:
- HistGradientBoostingClassifier cÃ³ xu hÆ°á»›ng **ráº¥t tá»± tin** (mean confidence ~0.95) trÃªn dá»¯ liá»‡u AQI
- ~62% unlabeled samples cÃ³ confidence â‰¥ 0.9
- â†’ Hyperparameter tuning cáº§n phÃ¹ há»£p vá»›i confidence distribution

![Confidence Distribution](images/13_DEBUG_confidence_distribution.png)

*HÃ¬nh 1: PhÃ¢n bá»‘ confidence scores cho tháº¥y model ráº¥t tá»± tin (mean=0.95)*

#### 3. Class imbalance cáº§n chiáº¿n lÆ°á»£c Ä‘áº·c biá»‡t

**Váº¥n Ä‘á»**:
- Baseline supervised: F1=0.0 cho class "Good" (hoÃ n toÃ n fail)
- Fixed threshold self-training: ThiÃªn lá»‡ch vá» lá»›p phá»• biáº¿n (Moderate, Unhealthy)

**Giáº£i phÃ¡p**:
- **Dynamic Threshold** (FlexMatch approach): +15.4% recall cho class "Hazardous"
- Class-specific threshold: Ï„_c = max(Ï„_base, p_model(c) / p_data(c))

![Dynamic Threshold Comparison](images/13_01_f1_macro_comparison.png)

*HÃ¬nh 2: Dynamic Threshold cáº£i thiá»‡n F1-macro vÃ  recall cho lá»›p hiáº¿m*

#### 4. Graph-based SSL: Accuracy cao nhÆ°ng khÃ´ng scalable

**Æ¯u Ä‘iá»ƒm**:
- Accuracy cao nháº¥t: F1-macro = 0.87 (+84% vs baseline)
- KhÃ´ng cáº§n iterative training
- Theoretical guarantees (convex optimization)

**Háº¡n cháº¿**:
- Memory intensive: O(nÂ²) similarity matrix
- Chá»‰ Ã¡p dá»¥ng Ä‘Æ°á»£c cho binary classification (vá»›i dataset nÃ y)
- KhÃ´ng scale vá»›i >100K samples

#### 5. Trade-offs quan trá»ng

```
Accuracy â†” Scalability â†” Memory â†” Training Time
```

- **Label Spreading**: Best accuracy, worst scalability
- **Co-Training**: Best label efficiency, 2Ã— training time
- **Self-Training**: Best balance cho production
- **Dynamic Threshold**: Best cho imbalanced & health-critical use case

### ğŸ“Š Káº¿t quáº£ chÃ­nh

#### Classification Performance (Multi-class: 6 AQI levels)

![Method Comparison](images/14_01_f1_macro_comparison.png)

*HÃ¬nh 3: So sÃ¡nh F1-macro cá»§a 6 phÆ°Æ¡ng phÃ¡p*

| Metric | Baseline | Self-Training | Co-Training | Dynamic Threshold |
|--------|----------|---------------|-------------|-------------------|
| **Accuracy** | 0.602 | 0.614 | 0.639 | 0.617 |
| **F1-Macro** | 0.472 | 0.680 | 0.710 | 0.685 |
| **Recall (Hazardous)** | 0.54 | 0.60 | 0.65 | **0.70** |
| **Training Time** | 1Ã— | 10Ã— | 20Ã— | 10Ã— |

#### Regression Performance (PM2.5 Prediction)

| Metric | Value | Interpretation |
|--------|-------|----------------|
| **RMSE** | 25.33 Î¼g/mÂ³ | Sai sá»‘ trung bÃ¬nh ~25 units |
| **MAE** | 12.32 Î¼g/mÂ³ | Sai sá»‘ tuyá»‡t Ä‘á»‘i ~12 units |
| **SMAPE** | 23.84% | Sai sá»‘ pháº§n trÄƒm ~24% |
| **RÂ²** | 0.949 | Model giáº£i thÃ­ch 94.9% variance |

![Actual vs Predicted PM2.5](images/07_actual_vs_predicted.png)

*HÃ¬nh 4: PM2.5 thá»±c táº¿ vs dá»± Ä‘oÃ¡n cho tháº¥y RÂ²=0.949*

#### ARIMA Forecasting (Single Station)

| Metric | Value | Note |
|--------|-------|------|
| **RMSE** | ~28 Î¼g/mÂ³ | Comparable vá»›i regression |
| **Forecast Horizon** | 168 hours (7 days) | Weekly ahead prediction |
| **Seasonal Pattern** | Detected | Hourly & daily cycles |

![ARIMA Forecast](images/08_forecast_vs_actual.png)

*HÃ¬nh 5: ARIMA forecast 7 ngÃ y vá»›i confidence intervals*

---

## ğŸ—‚ï¸ Cáº¥u trÃºc dá»± Ã¡n

- Nguá»“n: **Beijing Multiâ€‘Site Air Quality** (12 stations, dá»¯ liá»‡u theo giá»).
- Repo há»— trá»£ 2 cÃ¡ch náº¡p dá»¯ liá»‡u trong notebook `preprocessing_and_eda.ipynb`:
  - **(Khuyáº¿n nghá»‹ cho lá»›p há»c)** dÃ¹ng file ZIP local:
    - Ä‘áº·t file vÃ o `data/raw/PRSA2017_Data_20130301-20170228.zip`
    - set `USE_UCIMLREPO=False`
  - dÃ¹ng `ucimlrepo` (náº¿u notebook cÃ³ há»— trá»£ trong code): set `USE_UCIMLREPO=True`

> LÆ°u Ã½ â€œleakageâ€: **khÃ´ng dÃ¹ng trá»±c tiáº¿p `PM2.5` / `pm25_24h` trong feature Ä‘áº§u vÃ o cho mÃ´ hÃ¬nh phÃ¢n lá»›p AQI**.

---

## 2) Cáº¥u trÃºc thÆ° má»¥c

```
air_quality_timeseries_with_semi/
â”œâ”€ data/
â”‚  â”œâ”€ raw/                # ZIP dá»¯ liá»‡u gá»‘c
â”‚  â””â”€ processed/          # parquet + metrics + predictions + alerts
â”œâ”€ notebooks/
â”‚  â”œâ”€ preprocessing_and_eda.ipynb
â”‚  â”œâ”€ feature_preparation.ipynb
â”‚  â”œâ”€ classification_modelling.ipynb
â”‚  â”œâ”€ regression_modelling.ipynb
â”‚  â”œâ”€ arima_forecasting.ipynb
â”‚  â”œâ”€ semi_dataset_preparation.ipynb          
â”‚  â”œâ”€ semi_self_training.ipynb                
â”‚  â”œâ”€ semi_co_training.ipynb                  
â”‚  â”œâ”€ semi_supervised_report.ipynb            
â”‚  â””â”€ runs/                                   # output notebooks khi cháº¡y papermill
â”œâ”€ src/
â”‚  â”œâ”€ classification_library.py
â”‚  â”œâ”€ regression_library.py
â”‚  â”œâ”€ timeseries_library.py
â”‚  â””â”€ semi_supervised_library.py              
â”œâ”€ run_papermill.py
â”œâ”€ requirements.txt
â””â”€ README.md
```

---

## 3) CÃ i Ä‘áº·t mÃ´i trÆ°á»ng

### 3.1 Táº¡o mÃ´i trÆ°á»ng (Conda) vÃ  kernel cho Papermill
Repo máº·c Ä‘á»‹nh cháº¡y papermill vá»›i kernel tÃªn **`beijing_env`** (xem `run_papermill.py`).

```bash
conda create -n beijing_env python=3.11 -y
conda activate beijing_env
pip install -r requirements.txt

# Ä‘Äƒng kÃ½ kernel Ä‘á»ƒ Papermill gá»i Ä‘Æ°á»£c
python -m ipykernel install --user --name beijing_env --display-name "beijing_env"
```

### 3.2 Kiá»ƒm tra nhanh
```bash
python -c "import pandas, sklearn, papermill; print('OK')"
```

---

## 4) Cháº¡y pipeline (Papermill)

Cháº¡y toÃ n bá»™ pipeline:

```bash
python run_papermill.py
```

Káº¿t quáº£:
- Notebook cháº¡y xong sáº½ náº±m á»Ÿ `notebooks/runs/*_run.ipynb`
- Artefacts náº±m á»Ÿ `data/processed/` (metrics, predictions, alerts, parquet)

---

## 5) Pipeline Notebooks - HÆ°á»›ng dáº«n & Káº¿t quáº£

### 5.1 Core Pipeline (Notebooks 01-09)

#### ğŸ““ 01 - Preprocessing & EDA
**ğŸ“– [Documentation](documents/01_preprocessing_and_eda.md)** | **ğŸ’» Notebook**: `notebooks/01_preprocessing_and_eda.ipynb`

**Má»¥c tiÃªu**: Load dá»¯ liá»‡u tá»« 12 tráº¡m, lÃ m sáº¡ch, táº¡o features thá»i gian vÃ  lag features

**Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c**:
- âœ… Processed **420,768 records** tá»« 12 stations (2013-2017)
- âœ… Missing rate: **PM2.5: 5.2%**, other pollutants: <3%
- âœ… Táº¡o **52 features**: pollutants, weather, temporal (hour_sin/cos, dow), lag (1h, 3h, 24h)
- âœ… Táº¡o nhÃ£n **AQI classification** dá»±a trÃªn PM2.5 24h average
- ğŸ“Š Output: `01_cleaned.parquet`, `01_class_distribution.csv`

---

#### ğŸ““ 02 - Semi-supervised Dataset Preparation
**ğŸ“– [Documentation](documents/02_semi_dataset_preparation.md)** | **ğŸ’» Notebook**: `notebooks/02_semi_dataset_preparation.ipynb`

**Má»¥c tiÃªu**: Giá»¯ láº¡i unlabeled data vÃ  giáº£ láº­p tÃ¬nh huá»‘ng thiáº¿u nhÃ£n (chá»‰ trong train set)

**Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c**:
- âœ… Giá»¯ nguyÃªn **100% test set cÃ³ nhÃ£n** (Ä‘Ã¡nh giÃ¡ cÃ´ng báº±ng)
- âœ… Giáº£ láº­p thiáº¿u nhÃ£n trong train: **90% samples â†’ unlabeled**
- âœ… Labeled ratio: **10%** (~40K labeled, ~360K unlabeled)
- âœ… Preserve class distribution trong labeled subset
- ğŸ“Š Output: `02_dataset_for_semi.parquet`

---

#### ğŸ““ 03 - Feature Preparation for Classification
**ğŸ“– [Documentation](documents/03_feature_preparation.md)** | **ğŸ’» Notebook**: `notebooks/03_feature_preparation.ipynb`

**Má»¥c tiÃªu**: Chuáº©n bá»‹ dataset cho supervised classification (loáº¡i bá» PM2.5 Ä‘á»ƒ trÃ¡nh leakage)

**Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c**:
- âœ… Loáº¡i bá» **PM2.5, pm25_24h** khá»i features (prevent leakage)
- âœ… Giá»¯ láº¡i **51 features**: PM10, SO2, NO2, CO, O3, weather, temporal, lags
- âœ… Categorical encoding: **wd (16 directions), station (12 locations)**
- âœ… Train/test split: **396,264 / 16,671** samples
- ğŸ“Š Output: `03_dataset_for_clf.parquet`, `03_feature_list.csv`

---

#### ğŸ““ 04 - Self-Training Semi-supervised Learning
**ğŸ“– [Documentation](documents/04_semi_self_training.md)** | **ğŸ’» Notebook**: `notebooks/04_semi_self_training.ipynb`

**Má»¥c tiÃªu**: Ãp dá»¥ng Self-Training Ä‘á»ƒ táº­n dá»¥ng unlabeled data

**Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c**:
- âœ… **F1-macro: 0.534** (baseline: 0.472) â†’ **+13.1% improvement**
- âœ… Accuracy: 0.589 (baseline: 0.602)
- âœ… Há»™i tá»¥ sau **10 iterations**
- âœ… Tá»•ng pseudo-labels: **~340K samples** Ä‘Æ°á»£c thÃªm vÃ o training
- âœ… Confidence threshold Ï„=0.9 Ä‘áº£m báº£o high-quality pseudo-labels
- ğŸ“Š Output: `04_metrics_self_training.json`, `04_alerts_self_training_sample.csv`
- ğŸ“ˆ Visualization: `images/04_self_training_dynamics.png`

---

#### ğŸ““ 05 - Co-Training (Dual-View SSL)
**ğŸ“– [Documentation](documents/05_semi_co_training.md)** | **ğŸ’» Notebook**: `notebooks/05_semi_co_training.ipynb`

**Má»¥c tiÃªu**: Co-Training vá»›i 2 views Ä‘á»™c láº­p (temporal+weather vs pollutants+lags)

**Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c**:
- âœ… **F1-macro: 0.404** (tháº¥p hÆ¡n baseline do view design chÆ°a optimal)
- âœ… Agreement mechanism: Chá»‰ chá»n samples mÃ  cáº£ 2 views Ä‘á»u confident
- âœ… View 1: Temporal & weather features
- âœ… View 2: Pollutants & lag features
- âš ï¸ **Lesson learned**: View design critical - cáº§n optimize further
- ğŸ“Š Output: `05_metrics_co_training.json`, `05_alerts_co_training_sample.csv`
- ğŸ“ˆ Visualization: `images/05_co_training_dynamics.png`

---

#### ğŸ““ 06 - Supervised Classification Baseline
**ğŸ“– [Documentation](documents/06_classification_modelling.md)** | **ğŸ’» Notebook**: `notebooks/06_classification_modelling.ipynb`

**Má»¥c tiÃªu**: Thiáº¿t láº­p baseline supervised learning (chá»‰ dÃ¹ng labeled data)

**Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c**:
- âœ… **F1-macro: 0.472** - ÄÃ¢y lÃ  baseline reference
- âœ… **Accuracy: 0.602** (60.2% correct predictions)
- âŒ **F1-score (Good): 0.0** - Model hoÃ n toÃ n fail vá»›i class "Good"
- âœ… **F1-score (Moderate): 0.71** - Best performance (lá»›p phá»• biáº¿n nháº¥t)
- âœ… Model: HistGradientBoostingClassifier
- ğŸ“Š Output: `06_metrics.json`, `06_classification_report.csv`
- ğŸ“ˆ Visualization: `images/06_confusion_matrix.png`

**ğŸ’¡ Key Insight**: Class imbalance nghiÃªm trá»ng â†’ cáº§n SSL hoáº·c Dynamic Threshold

---

#### ğŸ““ 07 - PM2.5 Regression Prediction
**ğŸ“– [Documentation](documents/07_regression_modelling.md)** | **ğŸ’» Notebook**: `notebooks/07_regression_modelling.ipynb`

**Má»¥c tiÃªu**: Dá»± bÃ¡o PM2.5 (continuous value) thay vÃ¬ classification

**Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c**:
- âœ… **RÂ² = 0.949** - Model giáº£i thÃ­ch 94.9% variance (excellent!)
- âœ… **RMSE: 25.33 Î¼g/mÂ³** - Sai sá»‘ trung bÃ¬nh cháº¥p nháº­n Ä‘Æ°á»£c
- âœ… **MAE: 12.32 Î¼g/mÂ³** - Median absolute error
- âœ… **SMAPE: 23.84%** - Symmetric percentage error
- âœ… Features: PM10, SO2, NO2, CO, O3, weather, temporal, lags (52 features)
- ğŸ“Š Output: `07_regression_metrics.json`, `07_regressor.joblib`, `07_regression_predictions.csv`
- ğŸ“ˆ Visualization: `images/07_actual_vs_predicted.png`, `images/07_target_distribution.png`

**ğŸ’¡ Key Insight**: Regression hoáº¡t Ä‘á»™ng ráº¥t tá»‘t, cÃ³ thá»ƒ dÃ¹ng Ä‘á»ƒ convert thÃ nh AQI classification

---

#### ğŸ““ 08 - ARIMA Time Series Forecasting
**ğŸ“– [Documentation](documents/08_arima_forecasting.md)** | **ğŸ’» Notebook**: `notebooks/08_arima_forecasting.ipynb`

**Má»¥c tiÃªu**: ARIMA forecast cho 1 tráº¡m (Aotizhongxin), dá»± bÃ¡o 7 ngÃ y

**Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c**:
- âœ… **Forecast horizon: 168 hours** (7 days ahead)
- âœ… **RMSE: ~28 Î¼g/mÂ³** (comparable vá»›i regression)
- âœ… PhÃ¡t hiá»‡n **seasonal patterns**: Hourly & daily cycles
- âœ… ACF/PACF analysis: Significant autocorrelation up to lag 48h
- âœ… Model: Auto ARIMA with seasonal decomposition
- ğŸ“Š Output: `08_arima_pm25_summary.json`, `08_arima_pm25_predictions.csv`, `08_arima_pm25_model.pkl`
- ğŸ“ˆ Visualizations: `images/08_forecast_vs_actual.png`, `images/08_acf_plot.png`, `images/08_hourly_seasonality.png`

**ğŸ’¡ Key Insight**: ARIMA captures seasonality tá»‘t nhÆ°ng struggle vá»›i sudden pollution spikes

---

#### ğŸ““ 09 - Semi-supervised Learning Report
**ğŸ“– [Documentation](documents/09_semi_supervised_report.md)** | **ğŸ’» Notebook**: `notebooks/09_semi_supervised_report.ipynb`

**Má»¥c tiÃªu**: Tá»•ng há»£p vÃ  so sÃ¡nh táº¥t cáº£ cÃ¡c phÆ°Æ¡ng phÃ¡p (baseline vs SSL)

**Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c**:
- âœ… **Comparison table**: Supervised (0.472) vs Self-Training (0.534) vs Co-Training (0.404)
- âœ… **Best method: Self-Training** (+13.1% improvement)
- âœ… Alert generation theo tráº¡m vá»›i threshold "Unhealthy"
- âœ… Station-wise timeline analysis
- âœ… Top alerts identification
- ğŸ“Š Outputs: Comparison charts, alert timelines
- ğŸ“ˆ Visualizations: `images/09_supervised_vs_semi_supervised.png`, `images/09_self_training_station_timeline.png`

**ğŸ’¡ Key Insight**: Self-Training hiá»‡u quáº£ vá»›i 10% labeled data, Co-Training cáº§n optimize view design

---

### 5.2 Advanced Experiments (Notebooks 10-14)

#### ğŸ““ 10 - Self-Training Parameter Sweep
**ğŸ“– [Documentation](documents/10_question01.md)** | **ğŸ’» Notebook**: `notebooks/10_Question01.ipynb`

**Má»¥c tiÃªu**: Kháº£o sÃ¡t áº£nh hÆ°á»Ÿng cá»§a TAU, MAX_NEW_PER_ITER, MAX_ITER

**Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c**:
- âœ… **Optimal TAU: 0.7** (balance precision & coverage)
- âœ… **Optimal MAX_NEW: 100-150** (phá»¥ thuá»™c confidence distribution)
- âœ… **Optimal MAX_ITER: 10** (convergence without overfitting)
- âœ… **F1-macro: 0.680** vá»›i cáº¥u hÃ¬nh tá»‘t nháº¥t
- âœ… PhÃ¡t hiá»‡n: Model cÃ³ **mean confidence = 0.95** (ráº¥t tá»± tin!)
- âœ… **~62% unlabeled samples** cÃ³ confidence â‰¥ 0.9
- ğŸ“Š Outputs: `10_01` Ä‘áº¿n `10_04_metrics_self_training.json`
- ğŸ“ˆ Visualizations: `images/10_01_f1_macro_comparison.png`, `images/10_03_f1_macro_line_chart.png`

**ğŸ’¡ Key Insight**: Confidence distribution áº£nh hÆ°á»Ÿng lá»›n â†’ cáº§n analyze trÆ°á»›c khi tune hyperparameters

---

#### ğŸ““ 11 - Co-Training Parameter Sweep
**ğŸ“– [Documentation](documents/11_question02.md)** | **ğŸ’» Notebook**: `notebooks/11_Question02.ipynb`

**Má»¥c tiÃªu**: Test 18 cáº¥u hÃ¬nh Co-Training (TAU Ã— MAX_NEW Ã— K_BEST)

**Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c**:
- âœ… **Best F1-macro: 0.710** (TAU=0.7, MAX_NEW=100, K_BEST=100)
- âœ… **Improvement: +50.4% vs baseline** (0.472 â†’ 0.710)
- âœ… **Improvement: +4.4% vs Self-Training** (0.680 â†’ 0.710)
- âœ… View design: Temporal+Weather (View 1) vs Pollutants+Lags (View 2)
- âœ… Agreement rate: ~75% trÃªn high-confidence samples
- âœ… Training time: **2Ã— Self-Training**
- ğŸ“Š Outputs: `11_01` Ä‘áº¿n `11_18_metrics_co_training.json`
- ğŸ“ˆ Visualizations: `images/11_01_co_training_default.png`, `images/11_02_co_training_line_chart.png`

**ğŸ’¡ Key Insight**: Co-Training best cho label efficiency, nhÆ°ng cáº§n thiáº¿t káº¿ views carefully

---

#### ğŸ““ 12 - Graph-based SSL (Label Propagation/Spreading)
**ğŸ“– [Documentation](documents/12_question03.md)** | **ğŸ’» Notebook**: `notebooks/12_Question03.ipynb`

**Má»¥c tiÃªu**: So sÃ¡nh Label Propagation vs Label Spreading (graph-based methods)

**Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c**:
- âœ… **Label Spreading F1-macro: 0.870** (best accuracy!)
- âœ… **Label Propagation F1-macro: 0.860**
- âœ… **Improvement: +84.3% vs baseline** (0.472 â†’ 0.870)
- âš ï¸ **Binary classification only** (Healthy vs Unhealthy) do memory constraint
- âš ï¸ **Memory intensive**: O(nÂ²) similarity matrix
- âš ï¸ **Not scalable** vá»›i >100K samples
- âœ… N_NEIGHBORS=7, ALPHA=0.2 (Label Spreading)
- ğŸ“Š Output: `12_01_graph_metrics.json`
- ğŸ“ˆ Visualization: `images/12_01_graph_based_comparison.png`

**ğŸ’¡ Key Insight**: Highest accuracy nhÆ°ng khÃ´ng scalable - chá»‰ dÃ¹ng cho research/small datasets

---

#### ğŸ““ 13 - Dynamic Threshold Self-Training (FlexMatch)
**ğŸ“– [Documentation](documents/13_question04.md)** | **ğŸ’» Notebook**: `notebooks/13_Question04.ipynb`

**Má»¥c tiÃªu**: So sÃ¡nh Fixed vs Dynamic Threshold cho class imbalance

**Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c**:
- âœ… **Dynamic Threshold F1-macro: 0.685** (vs Fixed: 0.680)
- âœ… **Recall (Hazardous): +15.4%** (0.52 â†’ 0.70) - Critical improvement!
- âœ… **Recall (Very_Unhealthy): +6.9%** (0.58 â†’ 0.62)
- âœ… **Recall (Good): +160%** (0.15 â†’ 0.39)
- âœ… Formula: Ï„_c = max(Ï„_base, p_model(c) / p_data(c))
- âœ… Debug analysis: PhÃ¡t hiá»‡n flat lines do MAX_NEW=100 bottleneck
- ğŸ“Š Outputs: Metrics for fixed vs dynamic with different tau values
- ğŸ“ˆ Visualizations: `images/13_01_f1_macro_comparison.png`, `images/13_02_f1_per_class.png`, `images/13_DEBUG_confidence_distribution.png`

**ğŸ’¡ Key Insight**: Dynamic Threshold tá»‘t nháº¥t cho health-critical applications (high recall cho lá»›p nguy hiá»ƒm)

---

#### ğŸ““ 14 - Comprehensive Baseline & Method Comparison
**ğŸ“– [Documentation](documents/14_baseline_and_comparison.md)** | **ğŸ’» Notebook**: `notebooks/14_Baseline_And_Comparison.ipynb`

**Má»¥c tiÃªu**: So sÃ¡nh toÃ n diá»‡n 6 phÆ°Æ¡ng phÃ¡p ML

**Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c**:
- âœ… **6 methods compared**: Baseline, Self-Training, Co-Training, Label Propagation, Label Spreading, Dynamic Threshold
- âœ… **Best overall: Label Spreading** (F1=0.870, +84.3%)
- âœ… **Best scalable: Co-Training** (F1=0.710, +50.4%)
- âœ… **Best for imbalance: Dynamic Threshold** (best recall cho rare classes)
- âœ… **Decision tree** cho method selection
- âœ… Performance vs cost trade-off analysis
- ğŸ“Š Output: `14_comparison_summary.csv`
- ğŸ“ˆ Visualizations: `images/14_01_f1_macro_comparison.png`, `images/14_02_f1_heatmap.png`, `images/14_03_performance_vs_cost.png`, `images/14_04_improvement_percentage.png`

**ğŸ’¡ Key Insight**: KhÃ´ng cÃ³ "best method" cho má»i case - phá»¥ thuá»™c constraints (data size, memory, accuracy requirement)

---

### 5.3 Tá»•ng káº¿t Performance

| Method | F1-Macro | vs Baseline | Scalability | Use Case |
|--------|----------|-------------|-------------|----------|
| **Baseline Supervised** | 0.472 | - | âœ… Excellent | Äá»§ labeled data |
| **Self-Training** | 0.680 | +44.1% | âœ… Excellent | General purpose |
| **Co-Training** | 0.710 | +50.4% | âœ… Good | Best label efficiency |
| **Label Propagation** | 0.860 | +82.2% | âŒ Poor | Small datasets only |
| **Label Spreading** | 0.870 | +84.3% | âŒ Poor | Best accuracy, binary |
| **Dynamic Threshold** | 0.685 | +45.1% | âœ… Excellent | Imbalanced + health-critical |

---

## 6) ThÆ° viá»‡n OOP (src/)

### 6.1 `src/classification_library.py`
- `time_split(df, cutoff)`: chia train/test theo thá»i gian
- `train_classifier(train_df, test_df, target_col='aqi_class')` â†’ tráº£ vá» `{model, metrics, pred_df}`
- Guard leakage: loáº¡i cá»™t nhÆ° `PM2.5`, `pm25_24h`, `datetime` khá»i features.

### 6.2 `src/semi_supervised_library.py` 
- `mask_labels_time_aware(...)`: giáº£ láº­p thiáº¿u nhÃ£n **chá»‰ trong TRAIN**
- `SelfTrainingAQIClassifier`: vÃ²ng láº·p pseudoâ€‘label theo ngÆ°á»¡ng `tau`
- `CoTrainingAQIClassifier`: coâ€‘training 2 views + lateâ€‘fusion
- `add_alert_columns(...)`: táº¡o `is_alert` theo ngÆ°á»¡ng má»©c AQI (vd tá»« `"Unhealthy"`)

---

## 7) MINI PROJECT: Semiâ€‘Supervised AQI + Alerts theo tráº¡m

### 7.1 Má»¥c tiÃªu
XÃ¢y dá»±ng há»‡ thá»‘ng:
- dá»± Ä‘oÃ¡n `aqi_class` cho tá»«ng timestamp/tráº¡m
- sinh **cáº£nh bÃ¡o** theo tráº¡m (`is_alert`)
- khi **thiáº¿u nhÃ£n AQI** (hoáº·c nhÃ£n khÃ´ng chuáº©n), dÃ¹ng **Selfâ€‘Training** vÃ  **Coâ€‘Training** Ä‘á»ƒ cáº£i thiá»‡n cháº¥t lÆ°á»£ng.

### 7.2 Thiáº¿t káº¿ thÃ­ nghiá»‡m (báº¯t buá»™c)
1) **Baseline supervised**  
   - Cháº¡y `classification_modelling.ipynb`  
   - Láº¥y `accuracy`, `f1_macro` tá»« `data/processed/metrics.json`

2) **Giáº£ láº­p thiáº¿u nhÃ£n (trainâ€‘only)**  
   - Cháº¡y `semi_dataset_preparation.ipynb` vá»›i:
     - `LABEL_MISSING_FRACTION âˆˆ {0.7, 0.9, 0.95, 0.98}`

3) **Selfâ€‘Training**  
   - Cháº¡y `semi_self_training.ipynb` vá»›i:
     - `TAU âˆˆ {0.8, 0.9, 0.95}`
   - PhÃ¢n tÃ­ch: vÃ²ng láº·p nÃ o báº¯t Ä‘áº§u â€œbÃ£o hoÃ â€, sá»‘ pseudoâ€‘labels tÄƒng/giáº£m ra sao.

4) **Coâ€‘Training**  
   - Cháº¡y `semi_co_training.ipynb` vá»›i `TAU` giá»‘ng Selfâ€‘Training
   - Báº¯t buá»™c thá»­ 2 cháº¿ Ä‘á»™:
     - **Auto split views** (Ä‘á»ƒ `VIEW1_COLS=None`, `VIEW2_COLS=None`)
     - **Manual views**: tá»± thiáº¿t káº¿ 2 views vÃ  giáº£i thÃ­ch vÃ¬ sao há»£p lÃ½.




Cháº¡y toÃ n bá»™ pipeline tá»± Ä‘á»™ng:

```bash
python run_papermill.py
```

**Káº¿t quáº£**:
- Output notebooks: `notebooks/runs/*_run.ipynb`
- Processed data: `data/processed/*.parquet`
- Metrics: `data/processed/*_metrics.json`
- Visualizations: `images/*.png`

### Option 2: Manual (Jupyter)

Cháº¡y tá»«ng notebook theo thá»© tá»± trong Jupyter:

```bash
jupyter notebook
```

Thá»© tá»± thá»±c hiá»‡n: 01 â†’ 02 â†’ 03 â†’ 04/05/06/07/08 (parallel) â†’ 09 â†’ 10/11/12/13 (optional) â†’ 14

### Option 3: Quick Test (Sample Data)

Äá»ƒ test nhanh vá»›i 10% data:

```python
# Trong notebook, set SAMPLE_FRAC
SAMPLE_FRAC = 0.1  # Use 10% of data
```

---

## ğŸ’¡ BÃ i há»c vÃ  Insights

### 1. SSL Methods Selection Guide

**Decision tree**:

```
CÃ³ Ä‘á»§ labeled data (>10%)? 
â”œâ”€ YES â†’ Supervised Baseline (Ä‘á»§ tá»‘t)
â””â”€ NO â†’ Continue

Dataset size < 50K?
â”œâ”€ YES â†’ Label Spreading (best accuracy, binary)
â””â”€ NO â†’ Continue

CÃ³ thá»ƒ thiáº¿t káº¿ 2 independent views?
â”œâ”€ YES â†’ Co-Training (+50% improvement)
â””â”€ NO â†’ Continue

Class imbalance nghiÃªm trá»ng?
â”œâ”€ YES â†’ Dynamic Threshold Self-Training
â””â”€ NO â†’ Standard Self-Training
```

### 2. Key Parameters Tuning

**Self-Training**:
- `TAU = 0.7`: Best balance giá»¯a precision vÃ  coverage
- `MAX_NEW_PER_ITER = 100-150`: Phá»¥ thuá»™c vÃ o confidence distribution
- `MAX_ITER = 10`: Äá»§ Ä‘á»ƒ há»™i tá»¥, avoid overfitting

**Co-Training**:
- `TAU = 0.7`: Giá»‘ng Self-Training
- `K_BEST â‰ˆ MAX_NEW_PER_VIEW`: Ensure quality
- View design: Temporal+Weather vs Pollutants+Lags

**Graph-based**:
- `N_NEIGHBORS = 7`: sqrt(n) hoáº·c log(n)
- `ALPHA = 0.2`: Label Spreading regularization

**Dynamic Threshold**:
- `TAU_BASE = 0.7`: Minimum threshold
- Class-specific Ï„_c tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh

### 3. Model Confidence Analysis is Critical

**Quan sÃ¡t**:
- HistGradientBoosting cÃ³ confidence ráº¥t cao (mean=0.95)
- Cáº§n analyze distribution trÆ°á»›c khi tune hyperparameters
- MAX_NEW_PER_ITER pháº£i phÃ¹ há»£p vá»›i sá»‘ lÆ°á»£ng high-confidence samples

**Recommendation**:
- LuÃ´n include debug cells Ä‘á»ƒ visualize confidence distribution
- Monitor validation metrics má»—i iteration
- Early stopping khi khÃ´ng cÃ²n improvement

### 4. Production Considerations

**Alert System Design**:
- Dynamic Threshold: Tá»‘t nháº¥t cho health-critical applications
- High recall cho lá»›p nguy hiá»ƒm quan trá»ng hÆ¡n overall accuracy
- Trade-off: False positives (over-alert) vs False negatives (miss critical)

**Scalability**:
- Self-Training/Co-Training: Scale tá»‘t vá»›i large dataset
- Graph-based: Chá»‰ dÃ¹ng cho research hoáº·c small-scale deployment
- Consider computational budget: 2Ã— training time vá»›i Co-Training

**Monitoring**:
- Track confidence distribution changes over time
- Detect distribution shift (p_data thay Ä‘á»•i)
- Retrain khi performance degradation

### 5. Research Directions

**Potential improvements**:
- [ ] Ensemble methods (Self-Training + Co-Training + Dynamic)
- [ ] Adaptive tau schedules (khÃ´ng chá»‰ fixed)
- [ ] Multi-station spatial features (leverage neighboring stations)
- [ ] Deep learning approaches (LSTM, Transformer)
- [ ] Active learning (chá»n samples quan trá»ng Ä‘á»ƒ label)

**Open questions**:
- Optimal labeled ratio cho tá»«ng method?
- View design automation cho Co-Training?
- Graph construction vá»›i mixed features (numeric + categorical)?
- Online learning support cho graph-based methods?

---

## ğŸ“– Keywords & Concepts

**Machine Learning**:
- Semi-supervised Learning (SSL)
- Self-Training, Co-Training
- Label Propagation, Label Spreading
- Pseudo-labeling, Confidence threshold
- Class imbalance, Dynamic threshold

**Air Quality**:
- PM2.5, AQI classification
- 6 AQI levels (Good â†’ Hazardous)
- Health implications
- Multi-station monitoring

**Technical**:
- HistGradientBoostingClassifier
- ARIMA time series
- Feature engineering (lag features, temporal features)
- kNN graph, Graph-based SSL
- Confusion matrix, F1-macro

**Methodologies**:
- Papermill pipeline automation
- Notebook-per-task design
- OOP libraries (src/)
- Time-based train/test split
- Validation set for iterative methods

---

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open Pull Request

**Areas for contribution**:
- New SSL methods (Tri-Training, MixMatch, FixMatch)
- Deep learning models
- Spatial features tá»« neighboring stations
- Real-time prediction API
- Dashboard visualization

---

## ğŸ“ Contact

**Project Maintainer**: ThanhTung-KHMT-1701

**GitHub**: [https://github.com/ThanhTung-KHMT-1701/AirGuard](https://github.com/ThanhTung-KHMT-1701/AirGuard)

**Issues**: [GitHub Issues](https://github.com/ThanhTung-KHMT-1701/AirGuard/issues)

---

## ğŸ“„ License

Dá»± Ã¡n nÃ y Ä‘Æ°á»£c phÃ¢n phá»‘i theo giáº¥y phÃ©p **MIT License** - xem file [LICENSE.txt](LICENSE.txt) Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.

**TÃ³m táº¯t**:
- âœ… Sá»­ dá»¥ng tá»± do cho má»¥c Ä‘Ã­ch cÃ¡ nhÃ¢n, há»c thuáº­t, thÆ°Æ¡ng máº¡i
- âœ… Sá»­a Ä‘á»•i vÃ  phÃ¢n phá»‘i láº¡i
- âœ… Sá»­ dá»¥ng trong dá»± Ã¡n nguá»“n Ä‘Ã³ng
- âš ï¸ Giá»¯ nguyÃªn thÃ´ng bÃ¡o báº£n quyá»n vÃ  license
- âš ï¸ KhÃ´ng cÃ³ báº£o hÃ nh

---

## ğŸ™ Acknowledgments

- **Dataset**: [UCI ML Repository - Beijing Multi-Site Air Quality Data](https://archive.ics.uci.edu/dataset/501/beijing+multi+site+air+quality+data)
- **Inspiration**: FlexMatch paper (NeurIPS 2021) cho Dynamic Threshold approach
- **Libraries**: scikit-learn, pandas, matplotlib, statsmodels
- **Community**: StackOverflow, GitHub discussions

---

## ğŸ“ˆ Project Statistics

- **Total Notebooks**: 14 (9 core + 5 advanced experiments)
- **Total Code**: ~5,000 lines (notebooks + src/)
- **Documentation**: ~15,000 words (14 markdown docs)
- **Experiments Run**: 50+ configurations tested
- **Visualizations**: 40+ charts created
- **Data Points**: 420,000+ hourly records
- **Classes**: 6 AQI levels
- **Stations**: 12 monitoring stations
- **Time Span**: 4 years (2013-2017)

---

<div align="center">

**â­ Náº¿u project nÃ y há»¯u Ã­ch, hÃ£y cho má»™t star! â­**

[â¬† Back to top](#airguard-beijing-air-quality-monitoring--prediction-system)

</div>
